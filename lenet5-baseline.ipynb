{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lenet-5 Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import default_collate\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Fashion MNIST Dataset and transform and normalise pixel value to between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the MNIST dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform= ToTensor())\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders with collate function for data augmentation using cutmix and mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "cutmix = v2.CutMix(num_classes=10)\n",
    "mixup = v2.MixUp(num_classes=10)\n",
    "cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])\n",
    "def collate_fn(batch):\n",
    "    return cutmix_or_mixup(*default_collate(batch))\n",
    "\n",
    "train_dataloader_augment = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn) # train data augment with Mixup and CutMix\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # normal training data\n",
    "test_dataloader = DataLoader(test_dataset, batch_size= batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Baseline LeNet5 CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, input_shape=(1, 28, 28), num_classes=10):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16 * 4 * 4, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, epochs, optimizer):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        # Calculate average training loss for the epoch\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = model(images)\n",
    "            loss = loss_fn(output, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average validation loss and accuracy for the epoch\n",
    "    avg_test_loss = test_loss / len(test_dataloader)\n",
    "    accuracy = 100 * (correct / total)\n",
    "\n",
    "    return avg_test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commence Training without data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.6635\n",
      "Epoch 2/50, Train Loss: 0.4351\n",
      "Epoch 3/50, Train Loss: 0.3672\n",
      "Epoch 4/50, Train Loss: 0.3316\n",
      "Epoch 5/50, Train Loss: 0.3068\n",
      "Epoch 6/50, Train Loss: 0.2890\n",
      "Epoch 7/50, Train Loss: 0.2727\n",
      "Epoch 8/50, Train Loss: 0.2570\n",
      "Epoch 9/50, Train Loss: 0.2447\n",
      "Epoch 10/50, Train Loss: 0.2347\n",
      "Epoch 11/50, Train Loss: 0.2237\n",
      "Epoch 12/50, Train Loss: 0.2160\n",
      "Epoch 13/50, Train Loss: 0.2066\n",
      "Epoch 14/50, Train Loss: 0.2006\n",
      "Epoch 15/50, Train Loss: 0.1925\n",
      "Epoch 16/50, Train Loss: 0.1856\n",
      "Epoch 17/50, Train Loss: 0.1771\n",
      "Epoch 18/50, Train Loss: 0.1728\n",
      "Epoch 19/50, Train Loss: 0.1668\n",
      "Epoch 20/50, Train Loss: 0.1613\n",
      "Epoch 21/50, Train Loss: 0.1565\n",
      "Epoch 22/50, Train Loss: 0.1492\n",
      "Epoch 23/50, Train Loss: 0.1450\n",
      "Epoch 24/50, Train Loss: 0.1400\n",
      "Epoch 25/50, Train Loss: 0.1342\n",
      "Epoch 26/50, Train Loss: 0.1314\n",
      "Epoch 27/50, Train Loss: 0.1255\n",
      "Epoch 28/50, Train Loss: 0.1209\n",
      "Epoch 29/50, Train Loss: 0.1168\n",
      "Epoch 30/50, Train Loss: 0.1140\n",
      "Epoch 31/50, Train Loss: 0.1089\n",
      "Epoch 32/50, Train Loss: 0.1067\n",
      "Epoch 33/50, Train Loss: 0.1024\n",
      "Epoch 34/50, Train Loss: 0.1004\n",
      "Epoch 35/50, Train Loss: 0.0974\n",
      "Epoch 36/50, Train Loss: 0.0924\n",
      "Epoch 37/50, Train Loss: 0.0891\n",
      "Epoch 38/50, Train Loss: 0.0891\n",
      "Epoch 39/50, Train Loss: 0.0870\n",
      "Epoch 40/50, Train Loss: 0.0841\n",
      "Epoch 41/50, Train Loss: 0.0781\n",
      "Epoch 42/50, Train Loss: 0.0798\n",
      "Epoch 43/50, Train Loss: 0.0749\n",
      "Epoch 44/50, Train Loss: 0.0728\n",
      "Epoch 45/50, Train Loss: 0.0727\n",
      "Epoch 46/50, Train Loss: 0.0709\n",
      "Epoch 47/50, Train Loss: 0.0689\n",
      "Epoch 48/50, Train Loss: 0.0680\n",
      "Epoch 49/50, Train Loss: 0.0642\n",
      "Epoch 50/50, Train Loss: 0.0653\n"
     ]
    }
   ],
   "source": [
    "train(train_dataloader, model, 50, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test trained model without data augmentation on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_test_loss, acc = test(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on base model: 88.68%\n",
      "Average test loss on base model: 0.6245700081065297%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on base model: \" + str(acc) + \"%\")\n",
    "print(\"Average test loss on base model: \" + str(avg_test_loss) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commence Training with MixUp and CutMix augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 1.4063\n",
      "Epoch 2/50, Train Loss: 1.2114\n",
      "Epoch 3/50, Train Loss: 1.1364\n",
      "Epoch 4/50, Train Loss: 1.1207\n",
      "Epoch 5/50, Train Loss: 1.0727\n",
      "Epoch 6/50, Train Loss: 1.0628\n",
      "Epoch 7/50, Train Loss: 1.0418\n",
      "Epoch 8/50, Train Loss: 1.0244\n",
      "Epoch 9/50, Train Loss: 1.0034\n",
      "Epoch 10/50, Train Loss: 1.0023\n",
      "Epoch 11/50, Train Loss: 0.9971\n",
      "Epoch 12/50, Train Loss: 0.9827\n",
      "Epoch 13/50, Train Loss: 0.9772\n",
      "Epoch 14/50, Train Loss: 0.9771\n",
      "Epoch 15/50, Train Loss: 0.9710\n",
      "Epoch 16/50, Train Loss: 0.9836\n",
      "Epoch 17/50, Train Loss: 0.9596\n",
      "Epoch 18/50, Train Loss: 0.9677\n",
      "Epoch 19/50, Train Loss: 0.9721\n",
      "Epoch 20/50, Train Loss: 0.9505\n",
      "Epoch 21/50, Train Loss: 0.9619\n",
      "Epoch 22/50, Train Loss: 0.9517\n",
      "Epoch 23/50, Train Loss: 0.9548\n",
      "Epoch 24/50, Train Loss: 0.9530\n",
      "Epoch 25/50, Train Loss: 0.9490\n",
      "Epoch 26/50, Train Loss: 0.9384\n",
      "Epoch 27/50, Train Loss: 0.9459\n",
      "Epoch 28/50, Train Loss: 0.9596\n",
      "Epoch 29/50, Train Loss: 0.9339\n",
      "Epoch 30/50, Train Loss: 0.9280\n",
      "Epoch 31/50, Train Loss: 0.9418\n",
      "Epoch 32/50, Train Loss: 0.9273\n",
      "Epoch 33/50, Train Loss: 0.9202\n",
      "Epoch 34/50, Train Loss: 0.9300\n",
      "Epoch 35/50, Train Loss: 0.9329\n",
      "Epoch 36/50, Train Loss: 0.9317\n",
      "Epoch 37/50, Train Loss: 0.9271\n",
      "Epoch 38/50, Train Loss: 0.9216\n",
      "Epoch 39/50, Train Loss: 0.9298\n",
      "Epoch 40/50, Train Loss: 0.9309\n",
      "Epoch 41/50, Train Loss: 0.9278\n",
      "Epoch 42/50, Train Loss: 0.9049\n",
      "Epoch 43/50, Train Loss: 0.9236\n",
      "Epoch 44/50, Train Loss: 0.9307\n",
      "Epoch 45/50, Train Loss: 0.9286\n",
      "Epoch 46/50, Train Loss: 0.9209\n",
      "Epoch 47/50, Train Loss: 0.9108\n",
      "Epoch 48/50, Train Loss: 0.9165\n",
      "Epoch 49/50, Train Loss: 0.9144\n",
      "Epoch 50/50, Train Loss: 0.9161\n"
     ]
    }
   ],
   "source": [
    "train(train_dataloader_augment, model, 50, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test trained model with MixUp and CutMix augmentation on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_test_loss, acc = test(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on base model: 89.81%\n",
      "Average test loss on base model: 0.3338014339655638\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on base model: \" + str(acc) + \"%\")\n",
    "print(\"Average test loss on base model: \" + str(avg_test_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
